[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m currently a PhD student at The University of North Carolina at Charlotte working under the supervision of Dr. Minwoo Lee. I am interested in machine learning and computer vision, particularly my research is focused on the subfields of machine learning i.e., few-shot learning, meta-learning and continual learning in neural networks.\n","date":1646686729,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646686729,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ArchitParnami.github.io/author/archit-parnami/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/archit-parnami/","section":"authors","summary":"I\u0026rsquo;m currently a PhD student at The University of North Carolina at Charlotte working under the supervision of Dr. Minwoo Lee. I am interested in machine learning and computer vision, particularly my research is focused on the subfields of machine learning i.","tags":null,"title":"Archit Parnami","type":"authors"},{"authors":["Archit Parnami"],"categories":["Few-Shot Learning"],"content":"","date":1646686729,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646686729,"objectID":"91af78d147e7eb1c7cbfe39cd87531cc","permalink":"https://ArchitParnami.github.io/publication/03-fsl-survey/","publishdate":"2022-03-07T16:00:00-04:00","relpermalink":"/publication/03-fsl-survey/","section":"publication","summary":"Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem). ","tags":["Few-Shot Learning","Meta-Learning","Survey"],"title":"Learning from Few Examples: A Summary of Approaches to Few-Shot Learning","type":"publication"},{"authors":["Archit Parnami"],"categories":["NLP"],"content":"","date":1637182729,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637182729,"objectID":"7a159f15cb43077ca38db8bfb7aec382","permalink":"https://ArchitParnami.github.io/publication/02-pruning/","publishdate":"2022-03-20T16:00:00-04:00","relpermalink":"/publication/02-pruning/","section":"publication","summary":"Recent years have seen a growing adoption of Transformer models such as BERT in Natural Language Processing and even in Computer Vision. However, due to their size, there has been limited adoption of such models within resource-constrained computing environments. This paper proposes novel pruning algorithm to compress transformer models by eliminating redundant Attention Heads. We apply the A* search algorithm to obtain a pruned model with strict accuracy guarantees. Our results indicate that the method could eliminate as much as 40% of the attention heads in the BERT transformer model with no loss in accuracy. ","tags":["Model Compression","Sentiment Analysis","Transformers"],"title":"Pruning Attention Heads of Transformer Models Using A* Search","type":"publication"},{"authors":["Archit Parnami"],"categories":["Graph"],"content":"","date":1637179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637179200,"objectID":"7e3b0efa8aa222f8d994fd40a92e1555","permalink":"https://ArchitParnami.github.io/publication/04-graph/","publishdate":"2021-11-17T16:00:00-04:00","relpermalink":"/publication/04-graph/","section":"publication","summary":"Accepted in KGCW'22: 3rd International Workshop on Knowledge Graph Construction, Co-located with the ESWC 2022, May 05-30-2022, Crete, Greece","tags":["Link Prediction","Social Networks","Graph Embedding"],"title":"Transformation of Node to Knowledge Graph Embeddings for Faster Link Prediction in Social Networks","type":"publication"},{"authors":["Archit Parnami"],"categories":["Paper Implementation"],"content":"","date":1596489192,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596489192,"objectID":"4aaf76d3d96cdd3a726474acd457372b","permalink":"https://ArchitParnami.github.io/project/dp-pix/","publishdate":"2020-08-03T17:13:12-04:00","relpermalink":"/project/dp-pix/","section":"project","summary":"","tags":["Differential Privacy","Image Pixelation"],"title":"Image Pixelization with Differential Privacy","type":"project"},{"authors":["Archit Parnami"],"categories":["Few-Shot Learning"],"content":"","date":1596488329,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596488329,"objectID":"b37f64d0487d4da8ec7983bd492f0b51","permalink":"https://ArchitParnami.github.io/publication/01-fs-kws/","publishdate":"2020-08-03T16:58:49-04:00","relpermalink":"/publication/01-fs-kws/","section":"publication","summary":"Accepted in ACM 2022 7th International Conference on Machine Learning Technologies (ICMLT 2022)","tags":["Keyword Spotting","Speech Recognition","Audio"],"title":"Few-Shot Keyword Spotting With Prototypical Networks","type":"publication"},{"authors":["Archit Parnami"],"categories":["Few-Shot-Learning"],"content":"   Paper  https://arxiv.org/abs/2006.15486     Code  https://github.com/imtiazziko/LaplacianShot    Main Idea   Transfer Learning: Image embeddings are obtained by pre-training a network on the set of base classes using cross-entropy loss.\n  Transductive Inference: Jointly classify all the query examples together.\n  Methodology Given a labeled support set $\\mathbb{X_s} = \\bigcup_{c=1}^C$ with C test classes, where each novel class $c$ has $|\\mathbb{X_s^c}|$ labeled examples. Ex., $|\\mathbb{X_s^c}| = 1$ for one-shot learning. The object is then to classify unlabeled unseen query sample set $|\\mathbb{X_q}|$.\nLet $f_{\\theta}$ be the embedding function with parameters $\\theta$. Then:\n$$x_q = f_{\\theta}(z_q) \\in \\mathbb{R}^M,$$ where $x_q$ is the encoding of the data point $z_q$.\nFor each query point $x_q$, let there be a assignment vector $\\mathbf{y_q} = [y_{q,1}, \u0026hellip;, y_{q,C}]^t \\in {0,1}^C$\nso that binary $y_{q,c}$ is equal to 1 if $x_q$ belongs to class $c$. Then $\\mathbf{Y}$ denotes the $N \\times C$ matrix whose rows are formed by $\\mathbf{y}_q$, where $N$ is the number of query points in $\\mathbb{X_q}$. Then the objective to minimize at inference is given by (to find a $\\mathbf{Y}$ such that):\n$$\\mathcal{E}(\\mathbf{Y}) = \\mathcal{N}(\\mathbf{Y}) + \\frac{\\lambda}{2} \\mathcal{L}(\\mathbf{Y})$$\nwhere\n$$\\mathcal{N}(\\mathbf{Y}) = \\sum_{q=1}^N \\sum_{c=1}^C y_{q,c} d(x_q - \\mathbf{m}_c)$$\nand\n$$\\mathcal{L}(\\mathbf{Y}) = \\frac{1}{2} \\sum_{q,p} w(x_q, x_p) || \\mathbf{y}_q - \\mathbf{y}_p||^2$$\n $\\mathcal{N}(\\mathbf{Y})$ is minimized globally when each query point is assigned to the class of the nearest prototype $\\mathbf{m}_c$ from the support set using a distance metric $d(x_q, m_c)$. The Laplacian term (regularizer), $\\mathcal{L}(\\mathbf{Y})$, encourages nearby points $(x_p, x_q)$ in the label space to the same latent label assignment. $w$ is any similarity metric. $\\lambda$ is regularization parameter whose value is found by measuring performance on validation set.  Using an iterative bound optimization procedure (Ex., Expectation-Maximization (EM)), $\\mathcal{E}(\\mathbf{Y})$ is minimized and $\\mathbf{Y}$ is found. Their optimization procedure converges within 15 iterations. Please refer to the paper for details on optimization method.\nResults    Network dataset 1-shot 5-shot     ResNet-18 miniImageNet $72.11 \\pm 0.19$ $82.31 \\pm 0.14$   ResNet-18 tieredImageNet $78.98 \\pm 0.21$ $86.39 \\pm 0.16$   ResNet-18 CUB $80.96$ $88.68$   ResNet-18 miniImageNet $\\rightarrow$ CUB1 $55.46$ $66.33$    Cite / BibTex @article{Ziko2020LaplacianRF, title={Laplacian Regularized Few-Shot Learning}, author={Imtiaz Masud Ziko and Jose Dolz and {\\'E}ric Granger and Ismail Ben Ayed}, journal={ArXiv}, year={2020}, volume={abs/2006.15486} }    Cross Domain FSL: Training on miniImageNet and testing on CUB.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1596487158,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596487158,"objectID":"13e598ed44a571f2753b3c78eb855bfb","permalink":"https://ArchitParnami.github.io/post/laplacian-fsl/","publishdate":"2020-08-03T16:39:18-04:00","relpermalink":"/post/laplacian-fsl/","section":"post","summary":"Paper  https://arxiv.org/abs/2006.15486     Code  https://github.com/imtiazziko/LaplacianShot    Main Idea   Transfer Learning: Image embeddings are obtained by pre-training a network on the set of base classes using cross-entropy loss.","tags":["ICML","2020","Transductive","Transfer-Learning"],"title":"Laplacian Regularized Few-Shot Learning","type":"post"}]